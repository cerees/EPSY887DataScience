%\documentclass[handout]{beamer}
%\documentclass[handout,10pt,slidestop,mathserif]{beamer}
%\usepackage{pgfpages}
%\pgfpagesuselayout{2 on 1}
\documentclass[10pt,slidestop,mathserif,c]{beamer}
\usetheme{Madrid}
\usecolortheme{seahorse}

\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage[noae]{Sweave}
\usepackage{moreverb}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{Sweave}
%\SweaveOpts{prefix.string=figures/}

\newcommand{\putat}[3]{\begin{picture}(0,0)(0,0)\put(#1,#2){#3}\end{picture}}
  
\newenvironment{changemargin}[2]{%
  \begin{list}{}{%
    \setlength{\topsep}{0pt}%
    \setlength{\leftmargin}{#1}%
    \setlength{\rightmargin}{#2}%
    \setlength{\listparindent}{\parindent}%
    \setlength{\itemindent}{\parindent}%
    \setlength{\parsep}{\parskip}%
  }%
  \item[]}{\end{list}}

%% Define a new 'leo' style for the package that will use a smaller font.
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\tiny\ttfamily}}}
\makeatother

\title{Classification and Regression Trees}
\subtitle{EPSY 887: Data Science Institute}
\author[Jason Bryer]{Jason Bryer}
\institute[Jason.Bryer.org]{\url{https://github.com/jbryer/EPSY887DataScience}\\\href{mailto:jason@bryer.org}{jason@bryer.org}}
\date[Oct 7, 2014]{October 7, 2014}

\begin{document}
\SweaveOpts{concordance=TRUE}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Agenda}
       \tableofcontents[currentsection,currentsubsections]
   \end{frame}
}

<<echo=FALSE>>=
options(width=50)
#options(digits=3)
options(continue="   ")
options(warn=-1)

require(tree)
require(rpart)
require(party)
require(randomForest)
require(ggplot2)
require(mice)
require(ROCR)
require(TriMatch)

data(tutoring, package='TriMatch')
students <- tutoring

students$Treat <- TRUE
students[students$treat %in% c('Control'),]$Treat <- FALSE

# California house prices
calif <- read.table('../../Data/cadata.dat', header=TRUE)

load('../../Data/titanic3.sav') 
titanic3$survived <- as.integer(titanic3$survived)
titanic3$pclass <- as.integer(titanic3$pclass)

par(mfrow = c(1,1), xpd = NA) #To prevent text from being clipped
par(bg="white", mai=c(1.2,1.5,1,1))

@

\frame{\titlepage}

\begin{frame}
    \frametitle{Agenda}
    \tableofcontents
\end{frame}

\section{Overview}

\begin{frame}[fragile]
	\frametitle{Classification and Regression Trees (CART)}
    The goal of CART methods is to find best predictor in \textit{X} of some outcome, \textit{y}. CART methods do this recursively using the following procedures:
    \begin{enumerate}
        \item Find the best predictor in \textit{X} for \textit{y}.
        \item Split the data into two based upon that predictor.
        \item Repeat 1 and 2 with the split datasets until a stopping criteria has been reached.
    \end{enumerate}
    \pause
    There are a number of possible stopping criteria including:
    \begin{itemize}
        \item Only one data point remains.
        \item All data points have the same outcome value.
        \item No predictor can be found that sufficiently splits the data.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Recusrive Partitioning Logic of CART}
    Consider the scatterplot to the right with the following characteristics:
    \begin{columns}[c]
        \column{1.5in}
            \begin{itemize}
                \item Binary outcome, \textit{G}, coded ``A" or ``B".
                \item Two predictors, \textit{x} and \textit{z}
                \item The vertical line at \textit{z} = 3 creates the first partition.
                \item The double horizontal line at \textit{x} = -4 creates the second partition.
                \item The triple horizontal line at \textit{x} = 6 creates the third partition.
            \end{itemize}
        \column{4in}
            \includegraphics{Partitioning}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Tree Structure}
    \begin{columns}[c]
    \column{2in}
        \begin{itemize}
            \item The root node contains the full dataset.
            \item The data are split into two mutually exclusive pieces. Cases where $x > c_{i}$ go to the right, cases where $x <= c_{i}$ go to the left.
            \item Those that go to the left reach a terminal node.
            \item Those on the right are split into two mutually exclusive pieces. Cases where $z > c_{2}$ go to the right and terminal node 3; cases where $z <= c_{2}$ go to the left and terminal node 2.
        \end{itemize}
    \column{3in}
        \includegraphics[height=0.75\textheight, keepaspectratio]{TreeDiagram.png}
    \end{columns}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Sum of Squared Errors}
    The sum of squared errors for a tree \textit{T} is:
    $$S=\sum _{ c\in leaves(T) }^{  }{ \sum _{ i\in c }^{  }{ { (y-{ m }_{ c }) }^{ 2 } }  }$$
    Where, ${ m }_{ c }=\frac { 1 }{ n } \sum _{ i\in c }^{  }{ { y }_{ i } }$, the prediction for leaf \textit{c}.
    \pause
    \ \\ \ \\
    Or, alternatively written as:
    $$S=\sum _{ c\in leaves(T) }^{  }{ { n }_{ c }{ V }_{ c } }$$
    Where $V_{c}$ is the within-leave variance of leaf \textit{c}.
    \pause
    \ \\ \ \\
    Or goal then is to find splits that minimize \textit{S}.
\end{frame}

\begin{frame}[fragile]
	\frametitle{Advantages of CART Methods}
    \begin{itemize}
        \item Making predictions is fast.
        \item It is easy to understand what variables are important in making predictions.
        \item Trees can be grown with data containing missingness. For rows where we cannot reach a leaf node, we can still make a prediction by averaging the leaves in the sub-tree we do reach.
        \item The resulting model will inherently include interaction effects.
        \item There are many reliable algorithms available.
    \end{itemize}

\end{frame}


\section{Regression Trees}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Califonia Real Estate}
    In this example we will predict the median California house price from the house's longitude and latitude. 
<<>>=
names(calif)
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Califonia Real Estate: Tree 1}
<<rtree1,fig=TRUE,width=10,height=7,include=FALSE>>=
treefit <- tree(log(MedianHouseValue) ~ Longitude + Latitude, 
data=calif)
plot(treefit); text(treefit, cex=0.75)
@
\begin{center}
    \includegraphics{Slides-rtree1}
\end{center}
\end{frame}

\begin{frame}[fragile,containsverbatim,shrink=.8]
    \frametitle{Califonia Real Estate: Tree 1}
<<rtree2,fig=TRUE,width=10,height=8,include=FALSE,echo=FALSE>>=
price.deciles <- quantile(calif$MedianHouseValue, 0:9/9)
cut.prices <- cut(calif$MedianHouseValue, price.deciles, include.lowest=TRUE)
plot(calif$Longitude, calif$Latitude, col=grey(10:2/11)[cut.prices], pch=20, 
	 xlab="Longitude", ylab="Latitude")
partition.tree(treefit, ordvars=c("Longitude","Latitude"), add=TRUE)
@
\begin{center}
    \includegraphics{Slides-rtree2}
\end{center}
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Califonia Real Estate: Tree 1}
<<>>=
summary(treefit)
@
    \vfill
    Here ``deviance" is the mean squared error, or root-mean-square error of 0.41.
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Califonia Real Estate: Tree 2, Reduce Minimum Deviance}
    We can increase the fit but changing the stopping criteria with the \texttt{mindev} parameter.
<<>>=
treefit2 <- tree(log(MedianHouseValue) ~ Longitude + Latitude, 
				 data=calif, mindev=.001)
summary(treefit2)
@
    With the larger tree we now have a root-mean-square error of 0.32.
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Califonia Real Estate: Tree 2, Reduce Minimum Deviance}
<<rtree3,fig=TRUE,include=FALSE,width=10,height=8,echo=FALSE>>=
plot(treefit2); text(treefit2, cex=0.75)
@
   \begin{center}
    \includegraphics{Slides-rtree3}
    \end{center}
\end{frame}


\begin{frame}[fragile,containsverbatim]
    \frametitle{Califonia Real Estate: Tree 2, Reduce Minimum Deviance}
<<rtree3map,fig=TRUE,include=FALSE,width=10,height=8,echo=FALSE>>=
plot(calif$Longitude, calif$Latitude, col=grey(10:2/11)[cut.prices], 
pch=20, xlab="Longitude", ylab="Latitude")
partition.tree(treefit2, ordvars=c("Longitude","Latitude"), add=TRUE)
@
    \begin{center}
    \includegraphics{Slides-rtree3map}
    \end{center}
\end{frame}


\begin{frame}[fragile,containsverbatim]
    \frametitle{Califonia Real Estate: Tree 3, Include All Variables}
    However, we can get a better fitting model by including the other variables.
<<>>=
treefit3 <- tree(log(MedianHouseValue) ~ ., data=calif)
summary(treefit3)

@
    With all the available variables, the root-mean-square error is 0.11.
\end{frame}


\section{Classification Trees}

\begin{frame}[fragile]
    \frametitle{Titanic Example\footnote{\href{http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.html}{Data available from Vanderbilt University}}}
    \begin{description}
        \item[survived] Survived Titanic sinking
        \item[sex] Gender
        \item[pclass] Passenger class
        \item[age] Age at sailing
        \item[sibsp] Number of siblings or spouses aboard.
    \end{description}
<<>>=
names(titanic3)
@
\end{frame}

\begin{frame}[fragile,containsverbatim,shrink=.9]
    \frametitle{Classification using \texttt{rpart}}
<<rpart>>=
(titanic.rpart <- rpart(survived ~ pclass + sex + age + sibsp, 
data=titanic3))
@
\end{frame}


\begin{frame}[fragile,containsverbatim]
    \frametitle{Classification using \texttt{rpart}}
<<rpartfig,fig=TRUE,width=10,height=8>>=
plot(titanic.rpart); text(titanic.rpart, use.n=TRUE, cex=1)
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Classification using \texttt{tree}}
<<tree>>=
(titanic.tree <- tree(survived ~ pclass + sex + age + sibsp, 
data=titanic3))
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Classification using \texttt{tree}}
<<treefig,fig=TRUE,width=10,height=8>>=
plot(titanic.tree); text(titanic.tree, cex=1)
@
\end{frame}

\begin{frame}[fragile,containsverbatim,shrink=.9]
    \frametitle{Classification using \texttt{ctree}}
<<ctree>>=
(titanic.ctree <- ctree(factor(survived) ~ pclass + sex + age + sibsp, 
data=titanic3))
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Classification using \texttt{ctree}}
<<ctreefig,fig=TRUE,width=10,height=8>>=
plot(titanic.ctree)
@
\end{frame}


\begin{frame}[fragile,containsverbatim]
    \frametitle{Receiver Operating Characteristic (ROC) Graphs}
    \begin{columns}
    \column{3in}
    In a classification model, outcomes are either as positive (\textit{p}) or negative (\textit{n}). There are then four possible outcomes:
    \begin{description}
        \item[true positive (TP)] The outcome from a prediction is \textit{p} and the actual value is also \textit{p}
        \item[false positive (FP)] The actual value is \textit{n}
        \item[true negative TN)] Both the prediction outcome and the actual value are \textit{n}.
        \item[false negative (FN)] The prediction outcome is \textit{n} while the actual value is \textit{p}.
    \end{description}

    \column{2in}
    \includegraphics[height=\textwidth,keepaspectratio]{rocdiagram}
    \end{columns}

\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{ROC Space}
    \begin{center}
        \includegraphics[keepaspectratio,height=.9\textheight]{ROCspace}
    \end{center}

\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{ROCR}
    The \texttt{ROCR} package provides three functions to plot ROCs.
    
<<rocr1,fig=TRUE,width=5,height=5,include=FALSE,include=FALSE>>=
titanic.pred <- predict(titanic.ctree)
pred <- prediction(titanic.pred, as.integer(titanic3$survived))
perf <- performance(pred, measure="tpr", x.measure="fpr") 
plot(perf, colorize=TRUE, yaxis.at=c(0,0.5,0.8,0.9,1), 
yaxis.las=1)
lines(c(0,1), c(0,1), col="grey")
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{ROCR}
    \begin{center}
        \includegraphics[keepaspectratio,height=.9\textheight]{Slides-rocr1}
    \end{center}

\end{frame}



\begin{frame}[fragile,containsverbatim]
    \frametitle{New Student Outreach Example}
    One issue with CART methods is that in many instances the models may over fit the data. In this example we will examine a new student outreach program.
<<hsplot1,fig=TRUE,include=FALSE,width=10,height=8>>=
names(students)
rp = rpart(Treat ~ Age + Ethnicity + Military + Gender + 
Employment + Transfer + ESL + 
EdMother + EdFather + Income, data=students)
plot(rp); text(rp, cex=1, use.n=TRUE)
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{New Student Outreach Example}
    \begin{center}
    \includegraphics{Slides-hsplot1}
    \end{center}
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{New Student Outreach Example}
    \begin{columns}
    \column{2.0in}
    The \texttt{where} element contains the leaf node for each row used to grow the tree (note that for \texttt{ctree} use the \texttt{where} function). Over-fitting is generally not an issue in propensity score methods since we do not wish to generalize the results in phase I. However, we do need students from both the treatment and comparison groups in each leaf node.
\column{2.0in}
<<>>=
strata = factor(rp$where)
table(strata, students$Treat)
@
    \end{columns}
\end{frame}

\begin{frame}[fragile,containsverbatim,shrink=.8]
    \frametitle{New Student Outreach Example}
    Typically, you will want to select a tree size that minimizes the cross-validated error, the \texttt{xerror} column printed by \texttt{printcp}.
<<>>=
printcp(rp)
@
    
    Or we can extract the smallest complexity parameter for the smallest cross-validated error.
<<>>=
(cp4min <- rp$cptable[which.min(rp$cptable[,"xerror"]),"CP"])
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{New Student Outreach Example}
<<hsplot2,fig=TRUE,include=FALSE,width=10,height=8>>=
rp2 = prune(rp, cp=cp4min - .001)
plot(rp2); text(rp2, use.n=TRUE, all=FALSE)
@
\end{frame}

\begin{frame}[fragile,containsverbatim]
    \frametitle{New Student Outreach Example}
    \begin{center}
        \includegraphics{Slides-hsplot2}
    \end{center}
\end{frame}


\begin{frame}[fragile,containsverbatim,t]
    \frametitle{New Student Outreach Example}
    \begin{columns}
    \column[t]{2.0in}
    Crosstab of strata before pruning
<<>>=
strata = factor(rp$where)
table(strata, students$Treat)
@
    \column[t]{2.0in}
    Crosstab of strata after pruning
<<>>=
strata2 = factor(rp2$where)
table(strata2, students$Treat)
@
    \end{columns}
\end{frame}


\begin{frame}[fragile,containsverbatim]
    \frametitle{Titanic Revisited: Logistic Regression}
    
    Lets revisit the Titanic dataset and compare the tree method with logistic regression.
    First, we need to impute missing values. In this dataset only age is missing with about 20\% missing.
    
<<results=hide>>=
titanic.mice <- mice(titanic3[,c("pclass","sex","age","sibsp")])
titanic.complete <- cbind(survived=titanic3$survived, 
complete(titanic.mice, 5))
@
    Perform a logistic regression with the \texttt{glm} function.
<<>>=
titanic.glm <- glm(survived ~ pclass + sex + age + sibsp,
				   data=titanic.complete,
				   family=binomial(logit))
@
\end{frame}

\begin{frame}[fragile,containsverbatim,shrink=.8]
\frametitle{Titanic Revisited: Logistic Regression}
<<echo=FALSE,results=hide>>=
oldopt <- options(width=100)
@
<<>>=
summary(titanic.glm)
@
\end{frame}

\begin{frame}[fragile,containsverbatim,shrink=.8]
\frametitle{Titanic Revisited: Logistic Regression}
But from our tree methods it appears there is an interaction effect between class and gender.
<<>>=
titanic.glm2 <- glm(survived ~ pclass + sex + pclass:sex + age + sibsp,
data=titanic.complete, family=binomial(logit))
summary(titanic.glm2)
@
<<echo=FALSE,results=hide>>=
options(oldopt)
@
\end{frame}


\section{Ensemble Methods}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Ensemble Methods}
    
    Ensemble methods use multiple models that are combined by weighting, or averaging, each individual model to provide an overall estimate. Each model is a random sample of the sample. Common ensemble methods include:
    \begin{itemize}
        \item \textit{Boosting} - Each successive trees give extra weight to points incorreclty predicted by earlier trees. After all trees have been estimated, the prediction is determined by a weighted ``vote" of all predictions (i.e. results of each individual tree model).
        \item \textit{Bagging} - Each tree is estimated independent of other trees. A simple ``majority vote" is take for the prediction.
        \item \textit{Random Forests} - In addition to randomly sampling the data for each model, each split is selected from a random subset of all predictors.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Random Forests}
    The random forest algorithm works as follows:
    \begin{enumerate}
        \item Draw $n_{tree}$ bootstrap samples from the original data.
        \item For each bootstrap sample, grow an unpruned tree. At each node, randomly sample $m_{try}$ predictors and choose the best split among those predictors selected\footnote{Bagging is a special case of random forests where $m_{try} = p$, where \textit{p} is the number of predictors}.
        \item Predict new data by aggregating the predictions of the $n_{tree}$ trees (majority votes for classification, average for regression).
    \end{enumerate}
    \pause
    Error rates are obtained as follows:
    \begin{enumerate}
        \item At each bootstrap iteration predict data not in the bootstrap sample (what Breiman calls ``out-of-bag", or OOB, data) using the tree grown with the bootstrap sample.
        \item Aggregate the OOB predictions. On average, each data point would be out-of-bag 36\% of the times, so aggregate these predictions. The calculated error rate is called the OOB estimate of the error rate.
    \end{enumerate}

\end{frame}

\begin{frame}[fragile,containsverbatim,shrink=.8]
\frametitle{Random Forests: Titanic Revisited}
<<echo=FALSE,results=hide>>=
oldopt <- options(width=100)
@
<<>>=
set.seed(2112)
titanic.rf <- randomForest(factor(survived) ~ pclass + sex + age + sibsp, 
						   data=titanic.complete,
						   ntree=5000,
						   importance=TRUE)
print(titanic.rf)
importance(titanic.rf)
@
<<echo=FALSE,results=hide>>=
options(oldopt)
@
\end{frame}


\section{Discussion}

\begin{frame}[fragile,containsverbatim]
    \frametitle{Discussion}
    \begin{itemize}
        \item CART Methods for Propensity Score Analysis
        \begin{itemize}
            \item Overfitting?
            \item Stratification
        \end{itemize}
        \item CART Methods for Data Mining
        \begin{itemize}
            \item Splitting datasets
            \item Overfitting
        \end{itemize}
        \item Missing Data
        \item Results Informing other Regression Methods
        \item Ensemble Method
    \end{itemize}

\end{frame}



\begin{frame}[c]
	\LARGE{Thank You}\\
	\normalsize
	Jason Bryer (jason@bryer.org)\\
	\url{http://jason.bryer.org}
\end{frame}

\end{document}
